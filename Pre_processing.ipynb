{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util.shape import view_as_blocks\n",
    "from patchify import patchify, unpatchify\n",
    "import numpy as np\n",
    "import skimage\n",
    "import array\n",
    "from deepbrain import Extractor\n",
    "import cv2\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading mri and ground truth images\n",
    "file = 'c0002s0027t01'; # the name of the input MRI exam \n",
    "\n",
    "# path for brain mri\n",
    "path_mr = 'Brian Stroke Dataset/' + file + '.nii';\n",
    "\n",
    "# path for ground truth of brain mri\n",
    "path_lesion_mask = 'Brian Stroke Dataset/'+ file + '_LesionSmooth.nii'\n",
    "\n",
    "# Get nibabel image objects\n",
    "brain_nii = nib.load(path_mr) ;\n",
    "data_brain = brain_nii.get_fdata()\n",
    "\n",
    "mask_nii = nib.load(path_lesion_mask);\n",
    "data_mask = mask_nii.get_fdata();\n",
    "\n",
    "\n",
    "# Convert to numpy ndarray (dtype: uint16)\n",
    "brain = np.array(data_brain)\n",
    "mask = np.array(data_mask)\n",
    "\n",
    "# check the shape of the brain volume and mask volume have the same size or not \n",
    "if brain.shape == mask.shape: \n",
    "    print('The brin and mask have the same shape')   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meta_header(filename, meta_dict):\n",
    "    header = ''\n",
    "    # do not use tags = meta_dict.keys() because the order of tags matters\n",
    "    tags = ['ObjectType','NDims','BinaryData',\n",
    "       'BinaryDataByteOrderMSB','CompressedData','CompressedDataSize',\n",
    "       'TransformMatrix','Offset','CenterOfRotation',\n",
    "       'AnatomicalOrientation',\n",
    "       'ElementSpacing',\n",
    "       'DimSize',\n",
    "       'ElementType',\n",
    "       'ElementDataFile',\n",
    "       'Comment','SeriesDescription','AcquisitionDate','AcquisitionTime','StudyDate','StudyTime']\n",
    "    for tag in tags:\n",
    "        if tag in meta_dict.keys():\n",
    "            header += '%s = %s\\n'%(tag,meta_dict[tag])\n",
    "    f = open(filename,'w')\n",
    "    f.write(header)\n",
    "    f.close()\n",
    "    \n",
    "def dump_raw_data(filename, data):\n",
    "    \"\"\" Write the data into a raw format file. Big endian is always used. \"\"\"\n",
    "    #Begin 3D fix\n",
    "    data=data.reshape([data.shape[0],data.shape[1]*data.shape[2]])\n",
    "    #End 3D fix\n",
    "    rawfile = open(filename,'wb')\n",
    "    a = array.array('f')\n",
    "    for o in data:\n",
    "        a.fromlist(list(o))\n",
    "    #if is_little_endian():\n",
    "    #    a.byteswap()\n",
    "    a.tofile(rawfile)\n",
    "    rawfile.close()\n",
    "    \n",
    "def write_mhd_file(mhdfile, data, dsize):\n",
    "    assert(mhdfile[-4:]=='.mhd')\n",
    "    meta_dict = {}\n",
    "    meta_dict['ObjectType'] = 'Image'\n",
    "    meta_dict['BinaryData'] = 'True'\n",
    "    meta_dict['BinaryDataByteOrderMSB'] = 'False'\n",
    "    meta_dict['ElementType'] = 'MET_FLOAT'\n",
    "    meta_dict['NDims'] = str(len(dsize))\n",
    "    meta_dict['DimSize'] = ' '.join([str(i) for i in dsize])\n",
    "    meta_dict['ElementDataFile'] = os.path.split(mhdfile)[1].replace('.mhd','.raw')\n",
    "    write_meta_header(mhdfile, meta_dict)\n",
    "\n",
    "    pwd = os.path.split(mhdfile)[0]\n",
    "    if pwd:\n",
    "        data_file = pwd +'/' + meta_dict['ElementDataFile']\n",
    "    else:\n",
    "        data_file = meta_dict['ElementDataFile']\n",
    "\n",
    "    dump_raw_data(data_file, data)    \n",
    "    \n",
    "def save_lesion(location, patch):\n",
    "    np.save(str(location) + '.npy', patch)\n",
    "    write_mhd_file(str(location) + '.mhd', patch, patch.shape[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skull stripping using deepBrain (credit: https://pypi.org/project/deepbrain/)\n",
    "ext = Extractor()\n",
    "\n",
    "# `prob` will be a 3d numpy image containing probability of being brain tissue for each of the voxels in `img`\n",
    "prob = ext.run(brain) \n",
    "\n",
    "# mask can be obtained as:\n",
    "skull_mask = prob > 0.5\n",
    "idx = (skull_mask==0)\n",
    "skull = brain\n",
    "skull[idx] = skull_mask[idx];\n",
    "\n",
    "\n",
    "# save the extracted skull \n",
    "#skull_location = 'C:/Users/DELL/Desktop/Skulls/'+ file \n",
    "#save_lesion(skull_location, skull)     \n",
    "\n",
    "\n",
    "# normalize extracted skull by subtracting mean and dividing std \n",
    "# mean_skull =  skull.mean()\n",
    "# std_skull = skull.std()\n",
    "# normalized_skull  = (skull-mean_skull)/std_skull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir, anns_dir):\n",
    "    length = len(data_dir)\n",
    "    data_out = []\n",
    "    for i in range(length):\n",
    "        patch = np.load(data_dir[i])\n",
    "        label = np.load(anns_dir[i])\n",
    "        data_out.append((patch, label))\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform varitial mode decomposition (VMD) \n",
    "from vmdpy import VMD    #(https://pypi.org/project/vmdpy/)\n",
    "vmd_skull = []\n",
    "def decompose_VMD(skull_stripped_volume)\n",
    "    num_slices = skull_stripped_volume.shape[2]\n",
    "    for i in range(num_slices):\n",
    "        alpha = 1000  # moderate bandwidth constraint \n",
    "        tau = 0.5     # noise-tolerance (no strict fidelity enforcement)  \n",
    "        K = 5         # number of mode\n",
    "        DC = 1        # no DC part imposed  \n",
    "        init = 1      # initialize omegas uniformly \n",
    "        tol = K*1e-6\n",
    "        #. Run VMD \n",
    "        f= lesion_slice\n",
    "        u, u_hat, omega = VMD(f, alpha, tau, K, DC, init, tol)  \n",
    "        vmd_slice = u_tmp[3,:,:] # choose mode three\n",
    "        vmd_skull = [:,:,vmd_slice]\n",
    "    return vmd_skull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create overlapping patches from vmd skull\n",
    "height = vmd_skull.shape[0]\n",
    "width = vmd_skull.shape[1]\n",
    "depth = vmd_skull.shape[2]\n",
    "print('Image Height       : ',height)\n",
    "print('Image Width        : ',width)\n",
    "print('Number of Slices : ',depth)\n",
    "\n",
    "\n",
    "# before dividing to patches, pre-process the 3D image to get the equal patches \n",
    "\n",
    "patch_size = 64;\n",
    "\n",
    "patches_y,remainder_y = divmod(height,patch_size);\n",
    "patches_x,remainder_x = divmod(width,patch_size);\n",
    "patches_z,remainder_z = divmod(depth,patch_size);\n",
    "print('Number of patchces in y axis : ', patches_y)\n",
    "print('Number of patchces in x axis : ', patches_x)\n",
    "print('Number of patchces in z axis : ', patches_z)\n",
    "\n",
    "\n",
    "# after knowing the remainder in each dimension we try padding \n",
    "# if remainder_y or remainder_x or remainder_z is greater than zero\n",
    "# then perform padding \n",
    "\n",
    "if remainder_y!= 0:\n",
    "    padding_size_y = patch_size-remainder_y\n",
    "else:\n",
    "    padding_size_y = 0\n",
    "    \n",
    "if remainder_x!= 0:\n",
    "    padding_size_x = patch_size-remainder_x\n",
    "else:\n",
    "    padding_size_x = 0\n",
    "    \n",
    "if remainder_z!= 0:\n",
    "    padding_size_z = patch_size-remainder_z\n",
    "else:\n",
    "    padding_size_z = 0\n",
    "\n",
    "\n",
    "if remainder_y!= 0 or remainder_x != 0 or remainder_z != 0:    \n",
    "    print('Padding is required')\n",
    "    padded_skull = np.pad(vmd_skull,((0,padding_size_y),(0,padding_size_x),(0,padding_size_z)),'constant')\n",
    "    padded_mask = np.pad(mask,((0,padding_size_y),(0,padding_size_x),(0,padding_size_z)),'constant')\n",
    "    \n",
    "padded_skull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing patches by overlapping 10 voxels\n",
    "from numpy.lib import stride_tricks\n",
    "def cutup(data, blck, strd):\n",
    "    sh = np.array(data.shape)\n",
    "    blck = np.asanyarray(blck)\n",
    "    strd = np.asanyarray(strd)\n",
    "    nbl = (sh - blck) // strd + 1\n",
    "    strides = np.r_[data.strides * strd, data.strides]\n",
    "    dims = np.r_[nbl, blck]\n",
    "    data6 = stride_tricks.as_strided(data, strides=strides, shape=dims)\n",
    "    return data6#.reshape(-1, *blck)\n",
    "\n",
    "patches_skull = cutup(padded_skull,(patch_size,patch_size,patch_size), (54,54,54))\n",
    "patches_mask = cutup(padded_mask,(patch_size,patch_size,patch_size), (54,54,54))\n",
    "\n",
    "patches_brain_dimension = patches_skull.shape\n",
    "patches_mask_dimension = patches_mask.shape\n",
    "print('Dimension patches by patchify : ',patches_mask_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches_y = patches_brain_dimension[0]\n",
    "num_patches_x = patches_brain_dimension[1]\n",
    "num_patches_z = patches_brain_dimension[2]\n",
    "\n",
    "for row in range(num_patches_y): \n",
    "    for col in range(num_patches_x): \n",
    "        for dep in range(num_patches_z):\n",
    "\n",
    "            patch_index_str = str(row) * str(col) * str(dep)\n",
    "\n",
    "            current_patch_skull =  patches_skull[row][col][dep]  # reference number of the patch (eg- patch_id_1 = [row=1,col=1,depth=1])\n",
    "            current_patch_label = patches_mask[row][col][dep]    # reference number of the associated mask\n",
    "            #current_patch_label_overlay = current_patch_skull*current_patch_label\n",
    "\n",
    "\n",
    "            # patch trimming (if the current patch is only zero voxels, we will trim it out )\n",
    "            sum_ele_brain = np.sum(current_patch_skull) # sum of all elements in the current skull patch\n",
    "            sum_ele_mask = np.sum(current_patch_label) # sum of all elements in current patch label  \n",
    "\n",
    "\n",
    "            if (sum_ele_brain > 0 and sum_ele_mask > 0):\n",
    "                location_lesion = 'pre_processed dataset/Patch/'+ file + '_' +  patch_index_str\n",
    "                location_label = 'pre_processed dataset/Label/'+ file +'_' +  patch_index_str\n",
    "                #location_label_overlay = 'E:/Brin Stroke Detection/Experiment/Dataset/Stoke/LabelOverlay/'+ file +'_lable_patch_' +  patch_index_str \n",
    "                status = 'Stroke'\n",
    "                save_lesion(location_lesion, current_patch_skull)\n",
    "                save_lesion(location_label, current_patch_label)\n",
    "                #save_lesion(location_label_overlay, current_patch_label_overlay)\n",
    "            elif (sum_ele_brain > 0 and sum_ele_mask == 0):\n",
    "                location_lesion = 'pre_processed dataset/Patch/'+ file +'_' +  patch_index_str\n",
    "                location_label = 'pre_processed dataset/Label/'+ file +'_' +  patch_index_str \n",
    "                status ='Non-Stroke'\n",
    "                save_lesion(location_lesion, current_patch_skull)\n",
    "                save_lesion(location_label, current_patch_label)\n",
    "\n",
    "            else:\n",
    "                status = \"The patch is trimed out\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
