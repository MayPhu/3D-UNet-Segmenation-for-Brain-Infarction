{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "#import nrrd\n",
    "import scipy.ndimage\n",
    "import scipy.misc\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.python.framework import ops\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the directory of the data (overlapped patches and associated mask patches)\n",
    "\n",
    "patches = './pre_processed dataset/patches/*.npy'\n",
    "labels = './pre_processed dataset/labels_annotated/*.npy'   \n",
    "\n",
    "#patches = 'D:/Brin Stroke Detection/Experiment/31.1.2021D/Dataset/Stroke3/Patches/*.npy'\n",
    "#labels = 'D:/Brin Stroke Detection/Experiment/31.1.2021D/Dataset/Stroke3/Labels_Annotated/*.npy'      \n",
    "    \n",
    "patch_addrs = glob.glob(patches)\n",
    "labels_addrs = glob.glob(labels)\n",
    "\n",
    "\n",
    "# divide the data into 60% for train and 20% for validtion and 20% for testing\n",
    "\n",
    "# training split\n",
    "n_train = int(0.6*len(patch_addrs))\n",
    "n_val = int(0.2*len(patch_addrs))\n",
    "n_test = int(0.2*len(patch_addrs))\n",
    "\n",
    "data_train_dir = patch_addrs[0:n_train]\n",
    "anns_train_dir = labels_addrs[0:n_train]\n",
    "\n",
    "# validation split\n",
    "data_val_dir = patch_addrs[n_train + 1 : n_train + n_val]\n",
    "anns_val_dir = labels_addrs[n_train+ 1 : n_train + n_val]\n",
    "\n",
    "#testing split\n",
    "data_test_dir = patch_addrs[n_train + n_val + 1 :]\n",
    "anns_test_dir = labels_addrs[n_train + n_val + 1 :]\n",
    "\n",
    "print(os.path.basename(data_test_dir[1]).split('.')[0])\n",
    "os.path.basename(data_test_dir[1]).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to load the patches and corresponding mask patches\n",
    "def get_data(data_dir, anns_dir):\n",
    "    length = len(data_dir)\n",
    "    data_out = [] # for input patches\n",
    "    for i in range(length):\n",
    "        patch = np.load(data_dir[i])\n",
    "        label = np.load(anns_dir[i])\n",
    "        data_out.append((patch, label))\n",
    "        \n",
    "    return data_out\n",
    "\n",
    "def get_data_ref(data_dir, anns_dir):\n",
    "    length = len(data_dir)\n",
    "    data_ref = [] # for references\n",
    "    for i in range(length):\n",
    "        patch_ref = (os.path.basename(data_dir[1]).split('.')[0])\n",
    "        label_ref = (os.path.basename(anns_dir[1]).split('.')[0])\n",
    "        data_ref.append((patch_ref, label_ref))\n",
    "    return data_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset and save as pickle files\n",
    "\n",
    "train = get_data(data_train_dir, anns_train_dir)\n",
    "val = get_data(data_val_dir, anns_val_dir)\n",
    "test = get_data(data_test_dir, anns_test_dir)\n",
    "test_ref = get_data_ref(data_test_dir, anns_test_dir) \n",
    "\n",
    "# save the data into pickle files\n",
    "pickle.dump(file = open('./pickles/train.pkl', 'wb'), obj = train)\n",
    "pickle.dump(file = open('./pickles/val.pkl', 'wb'), obj = val)\n",
    "pickle.dump(file = open('./pickles/test.pkl', 'wb'), obj = test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of input and output of u-net \n",
    "\n",
    "input_size = 64    # width and height of input 3d patch\n",
    "input_depth = 64   # depth of input 3d patch\n",
    "output_size = 64   # width and height of output\n",
    "output_depth = 64  # depth of output 3d patch\n",
    "output_classes = 2 # number of output classes (two classes,i.e background and infarct lesion)\n",
    "\n",
    "# assign the hyperparameters to train u-net (need to tune to get higher performance)\n",
    "learning_rate = 0.001    \n",
    "num_epoches = 20       \n",
    "batch_size = 16 \n",
    "\n",
    "# location to save the weights, biases and \n",
    "save_path = \"./tf/\" \n",
    "logs_path = \"./tf_logs/\"\n",
    "\n",
    "# check wherether there is previously trained model in save_path directory\n",
    "load_model = True\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)\n",
    "model_name = 'model'     # can modify the model name to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_raw_sample(data, size):\n",
    "    x_y_data = random.sample(data, size)\n",
    "    return [x[0] for x in x_y_data], [y[1] for y in x_y_data]\n",
    "\n",
    "\n",
    "# takes raw data (x, y) and scales to match desired input and output sizes to feed into tensorflow\n",
    "# pads and normalises input and also moves axes around to orientation expected by tensorflow\n",
    "def get_scaled_input(data, min_i = input_size, min_o = output_size, depth = input_depth, \n",
    "                    depth_out = output_depth, image_fill = 0, \n",
    "                    label_fill = 0, n_classes = output_classes, norm_max = 500):  \n",
    "    \n",
    "    input_scale_factor = min_i/data[0].shape[0]\n",
    "    output_scale_factor = min_o/data[0].shape[0]\n",
    "\n",
    "    vox_zoom = none\n",
    "    lbl_zoom = none\n",
    "\n",
    "    if not input_scale_factor == 1:\n",
    "        vox_zoom = scipy.ndimage.interpolation.zoom(data[0], input_scale_factor, order = 1) \n",
    "        # order 1 is bilinear - fast and good enough\n",
    "    else:\n",
    "        vox_zoom = data[0]\n",
    "\n",
    "    if not output_scale_factor == 1:\n",
    "        lbl_zoom = scipy.ndimage.interpolation.zoom(data[1], output_scale_factor, order = 0) \n",
    "        # order 0 is nearest neighbours: very important as it ensures labels are scaled properly (and stay discrete)\n",
    "    else:\n",
    "        lbl_zoom = data[1]   \n",
    "\n",
    "    lbl_pad = label_fill*np.ones((min_o, min_o, depth_out - lbl_zoom.shape[-1]))\n",
    "    lbl_zoom = np.concatenate((lbl_zoom, lbl_pad), 2)\n",
    "    lbl_zoom = lbl_zoom[np.newaxis, :, :, :]\n",
    "    \n",
    "    vox_pad = image_fill*np.ones((min_i, min_i, depth - vox_zoom.shape[-1]))\n",
    "    vox_zoom = np.concatenate((vox_zoom, vox_pad), 2)\n",
    "    \n",
    "    max_val = np.max(vox_zoom)\n",
    "    if not np.max(vox_zoom) == 0:\n",
    "        vox_zoom = vox_zoom * norm_max/np.max(vox_zoom)\n",
    "        \n",
    "    vox_zoom = vox_zoom[np.newaxis, :, :, :]\n",
    "\n",
    "    vox_zoom = np.swapaxes(vox_zoom, 0, -1)\n",
    "    lbl_zoom = np.swapaxes(lbl_zoom, 0, -1)\n",
    "    # swap axes\n",
    "        \n",
    "    return vox_zoom, lbl_zoom\n",
    "\n",
    "def upscale_segmentation(lbl, shape_desired):\n",
    "    # returns scaled up label for a given input label and desired shape. required for mean iou calculation\n",
    "    \n",
    "    scale_factor = shape_desired[0]/lbl.shape[0]\n",
    "    lbl_upscale = scipy.ndimage.interpolation.zoom(lbl, scale_factor, order = 0)\n",
    "    # order 0 even more important here\n",
    "    lbl_upscale = lbl_upscale[:, :, :shape_desired[-1]]\n",
    "    if lbl_upscale.shape[-1] < shape_desired[-1]:\n",
    "        pad_zero = off_label_fill*np.zeros((shape_desired[0], shape_desired[1], shape_desired[2] - lbl_upscale.shape[-1]))\n",
    "        lbl_upscale = np.concatenate((lbl_upscale, pad_zero), axis = -1)\n",
    "    return lbl_upscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate loss\n",
    "def get_pred_iou(predictions, lbl_original, ret_full = False, reswap = False):\n",
    "    # get mean_iou for full batch\n",
    "    iou = []\n",
    "    dic_tmp=[]\n",
    "    for i in range(len(lbl_original)):\n",
    "        pred_cur = np.squeeze(predictions[i])\n",
    "        metric = get_mean_iou(pred_cur, lbl_original[i], ret_full = ret_full, reswap = reswap)\n",
    "        iou.append(metric)\n",
    "    if ret_full:\n",
    "        return np.mean(iou, axis = 0)\n",
    "    else:\n",
    "        return np.mean(iou)\n",
    "    \n",
    "def get_label_accuracy(pred, lbl_original):\n",
    "    # get pixel-wise labelling accuracy (demo metric)\n",
    "    \n",
    "    # swap axes back\n",
    "    pred = swap_axes(pred)\n",
    "    pred_upscale = upscale_segmentation(pred, np.shape(lbl_original))\n",
    "    return 100*np.sum(np.equal(pred_upscale, lbl_original))/np.prod(lbl_original.shape)\n",
    "\n",
    "def get_mean_iou(pred, lbl_original, num_classes = output_classes, ret_full = False, reswap = False):\n",
    "    # get mean iou between input predictions and target labels. note, method implicitly resizes as needed\n",
    "    # ret_full - returns the full iou across all classes\n",
    "    # reswap - if lbl_original is in tensorflow format, swap it back into the format expected by plotting tools (+ format of raw data)\n",
    "    \n",
    "    # swap axes back \n",
    "    pred = swap_axes(pred)\n",
    "    if reswap:\n",
    "        lbl_original = swap_axes(lbl_original)\n",
    "    pred_upscale = upscale_segmentation(pred, np.shape(lbl_original))\n",
    "    iou = [1]*num_classes\n",
    "    for i in range(num_classes): \n",
    "        test_shape = np.zeros(np.shape(lbl_original))\n",
    "        test_shape[pred_upscale == i] = 1\n",
    "        test_shape[lbl_original == i] = 1\n",
    "        full_sum = int(np.sum(test_shape))\n",
    "        test_shape = -1*np.ones(np.shape(lbl_original))\n",
    "        test_shape[lbl_original == i] = pred_upscale[lbl_original == i]\n",
    "        t_p = int(np.sum(test_shape == i))\n",
    "        if not full_sum == 0:\n",
    "            iou[i] = t_p/full_sum\n",
    "    if ret_full:\n",
    "        return iou\n",
    "    else: \n",
    "        return np.mean(iou)\n",
    "\n",
    "def get_dice (pred, lbl_original):\n",
    "    dice = np.sum(pred[lbl_original==1])*2.0 / (np.sum(pred) + np.sum(lbl_original))\n",
    "    return dice\n",
    "\n",
    "def get_dice_loss(y_true, y_pred):\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    y_pred_f = tf.cast(y_pred_f, dtype=tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    dice_loss = 1. - (2. * intersection + 1.) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + 1.)\n",
    "    return dice_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for unet\n",
    "class unetwork():\n",
    "    \n",
    "    # produces the conv_batch_relu combination as inf the paper\n",
    "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "    \n",
    "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
    "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
    "        conv = tf.nn.relu(conv) \n",
    "        return conv\n",
    "\n",
    "    # upconvolution - two different implementations: the first is as suggested in the original unet paper and the second is a more recent version\n",
    "    # needs to be determined if these do the same thing\n",
    "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "        # upsample_routine = tf.keras.layers.upsampling3d(size = (scale,scale,scale)) # uses tf.resize_images\n",
    "        # tensor = upsample_routine(tensor)\n",
    "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
    "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        # use_bias = false is a tensorflow bug\n",
    "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias= False, \n",
    "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
    "        return conv\n",
    "\n",
    "    def centre_crop_and_concat(self, prev_conv, up_conv):\n",
    "        # if concatenating two different sized tensors, centre crop the first tensor to the right size and concat\n",
    "        # needed if you don't have padding\n",
    "        p_c_s = prev_conv.get_shape()\n",
    "        u_c_s = up_conv.get_shape()\n",
    "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
    "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
    "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
    "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
    "        up_concat = tf.concat((prev_conv_crop, up_conv), 4)\n",
    "        return up_concat\n",
    "        \n",
    "    def __init__(self, base_filt = 8, in_depth = input_depth, out_depth = output_depth,\n",
    "                 in_size = input_size, out_size = output_size, num_classes = output_classes,\n",
    "                 learning_rate = learning_rate, print_shapes = True, drop = 0.2, should_pad = False):\n",
    "        # initialise your model with the parameters defined above\n",
    "        # print-shape is a debug shape printer for convenience\n",
    "        # should_pad controls whether the model has padding or not\n",
    "        # base_filt controls the number of base conv filters the model has. note deeper analysis paths have filters that are scaled by this value\n",
    "        # drop specifies the proportion of dropped activations\n",
    "        \n",
    "        self.base_init = tf.compat.v1.truncated_normal_initializer(stddev=0.1) # initialise weights  tf.\n",
    "        self.reg_init = tf.contrib.layers.l2_regularizer(scale=0.1) # initialise regularisation (was useful)\n",
    "        \n",
    "        self.should_pad = should_pad # to pad or not to pad, that is the question\n",
    "        self.drop = drop # set dropout rate\n",
    "        \n",
    "        with tf.variable_scope('3dunet'):\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.do_print = print_shapes\n",
    "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
    "            # define placeholders for feed_dict\n",
    "            self.model_labels = tf.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
    "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('input features shape', self.model_input.get_shape())\n",
    "                print('labels shape', labels_one_hot.get_shape())\n",
    "                \n",
    "            # level zero\n",
    "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
    "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
    "            # level one\n",
    "            max_1_1 = tf.layers.max_pooling3d(conv_0_2, [2,2,2], [2,2,2]) # stride, kernel previously [2,2,2]\n",
    "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
    "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_2 = tf.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
    "            # level two\n",
    "            max_2_1 = tf.layers.max_pooling3d(conv_1_2, [2,2,2], [2,2,2]) # stride, kernel previously [2,2,2]\n",
    "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
    "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
    "            conv_2_2 = tf.layers.dropout(conv_2_2, rate = self.drop, training = self.training)\n",
    "            # level three\n",
    "            max_3_1 = tf.layers.max_pooling3d(conv_2_2, [2,2,2], [2,2,2]) # stride, kernel previously [2,2,2]\n",
    "            conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
    "            conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
    "            conv_3_2 = tf.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
    "            # level two\n",
    "            up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [2,2,2]) # stride previously [2,2,2] \n",
    "            concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2)\n",
    "            conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
    "            conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
    "            conv_2_4 = tf.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
    "            # level one\n",
    "            up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [2,2,2]) # stride previously [2,2,2]\n",
    "            concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1)\n",
    "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = tf.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
    "            # level zero\n",
    "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [2,2,2])  # stride previously [2,2,2]\n",
    "            concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0)\n",
    "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = tf.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
    "            conv_out = tf.layers.conv3d(conv_0_4, output_classes, [1,1,1], [1,1,1], padding = 'same')\n",
    "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
    "            \n",
    "            # note, this can be more easily visualised in a tool like tensorboard; follows exact same format as in paper.\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('model convolution output shape', conv_out.get_shape())\n",
    "                print('model argmax output shape', self.predictions.get_shape())\n",
    "            \n",
    "            \n",
    "            y_pred = tf.squeeze([self.predictions])\n",
    "            y_true = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2) \n",
    "           \n",
    "            \n",
    "            \n",
    "            # calculate dice _loss\n",
    "            dice_loss = get_dice_loss(y_true, y_pred)                                          \n",
    "            self.loss = tf.reduce_mean(dice_loss) # assign model loss as dice_loss            \n",
    "            self.trainer = tf.train.adamoptimizer(learning_rate=learning_rate)           \n",
    "            \n",
    "            self.extra_update_ops = tf.get_collection(tf.graphkeys.update_ops) # ensure correct ordering for batch-norm to work\n",
    "            with tf.control_dependencies(self.extra_update_ops):\n",
    "                self.train_op = self.trainer.minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "unet = unetwork(drop = 0.2, base_filt = 10, should_pad = True) # model definition \n",
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.saver(tf.global_variables())\n",
    "config = tf.configproto()\n",
    "with tf.session(config=config) as sess:\n",
    "    writer = tf.summary.filewriter(logs_path, graph=tf.get_default_graph())\n",
    "    if load_model:\n",
    "        print('trying to load saved model...')\n",
    "        try:\n",
    "            print('loading from: ', save_path + 'model' + '.meta')\n",
    "            restorer = tf.train.import_meta_graph(save_path +'/'+ model_name+ '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_path))\n",
    "            print(\"model sucessfully restored\")\n",
    "        except ioerror:\n",
    "            sess.run(init)\n",
    "            print(\"no previous model found, running default init\") \n",
    "    t_loss = []\n",
    "    for i in range(num_epoches):\n",
    "        print('current iter: ', i, end='\\r')\n",
    "#         x, y, orig_y = get_dataset_sample(train, batch_size, no_perturb = true) (used if data-aug at runtime)\n",
    "        x, y = get_data_raw_sample(train_run, batch_size) # draw samples from batch\n",
    "        train_dict = {\n",
    "            unet.training: true,\n",
    "            unet.model_input: x,\n",
    "            unet.model_labels: y\n",
    "        }\n",
    "        _, loss = sess.run([unet.train_op, unet.loss], feed_dict = train_dict) # get loss\n",
    "        t_loss.append(loss) # loss store\n",
    "        if i % 400 == 0 and i > 0:\n",
    "            print('saving model at iter: ', i) # save periodically\n",
    "            saver.save(sess, save_path + model_name, global_step = i)\n",
    "        if i  == 20 and i > 0:\n",
    "            print('iteration', i, 'loss: ', np.mean(t_loss)) # get periodic progress reports\n",
    "            t_loss = []\n",
    "            iou_size = 5\n",
    "            x, y, orig_y = get_dataset_sample(train, iou_size) #(used if data-aug at runtime)\n",
    "            #x, y = get_data_raw_sample(train_run, batch_size) \n",
    "            train_dict = {\n",
    "                unet.training: false,\n",
    "                unet.model_input: x,\n",
    "                unet.model_labels: y\n",
    "            }\n",
    "            preds = np.squeeze(sess.run([unet.predictions], feed_dict = train_dict))\n",
    "            iou = get_pred_iou(preds, y, ret_full = true, reswap = true)\n",
    "            print('train iou (on scaled anns): ', iou, 'mean: ', np.mean(iou[:output_classes-1]))\n",
    "            \n",
    "            # validation\n",
    "            # get val mean iou over batch\n",
    "            x, y, orig_y = get_dataset_sample(val, iou_size, no_perturb = true)            \n",
    "            train_dict = {\n",
    "                unet.training: false,\n",
    "                unet.model_input: x,\n",
    "                unet.model_labels: y\n",
    "            }\n",
    "            preds = np.squeeze(sess.run([unet.predictions], feed_dict = train_dict))\n",
    "            iou = get_pred_iou(preds, orig_y, ret_full = true)\n",
    "            print('validation iou (on original anns): ', iou, 'mean: ', np.mean(iou[:output_classes-1]))\n",
    "            print('######################')            \n",
    "          \n",
    "    saver.save(sess,save_path + model_name, global_step = num_epoches) # final save[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing of the model\n",
    "\n",
    "test = pickle.load(file = open('./pickles/test.pkl', 'rb'))\n",
    "test_model_name = 'model.meta' # just for consistency - can ofc. be changed\n",
    "\n",
    "config = tf.configproto()\n",
    "test_predictions = []\n",
    "with tf.session(config=config) as sess:\n",
    "    print('loading saved model ...')\n",
    "#   try\n",
    "    restorer = tf.train.import_meta_graph(save_path + 'model-20'+ '.meta')\n",
    "    restorer.restore(sess, tf.train.latest_checkpoint(save_path))\n",
    "    print(\"model sucessfully restored\")\n",
    "    pred_out = []\n",
    "    y_orig = []\n",
    "    x_orig = []\n",
    "    x_in = []\n",
    "    y_in = []\n",
    "    i = 0\n",
    "    iou_out = []\n",
    "    dice_out=[]\n",
    "\n",
    "    while i < len(test):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for j in range(i, min(len(test), i + batch_size)):\n",
    "            y_orig.append(np.copy(test[j][1]))\n",
    "            x_orig.append(np.copy(test[j][0]))\n",
    "            x_cur, y_cur = get_scaled_input(test[j])\n",
    "            x_batch.append(x_cur)\n",
    "            y_batch.append(y_cur)\n",
    "        if len(x_batch) == 0: break\n",
    "        print('processing ', i)\n",
    "        x_in = x_in + x_batch\n",
    "        y_in = y_in + y_batch\n",
    "        test_dict = {\n",
    "            unet.training: false, # whether to perform batch-norm at inference (paper says this would be useful)\n",
    "            unet.model_input: x_batch,\n",
    "            unet.model_labels: y_batch\n",
    "        }\n",
    "        test_predictions = np.squeeze(sess.run([unet.predictions], feed_dict = test_dict))\n",
    "        if len(x_batch) == 1:\n",
    "            pred_out.append(test_predictions)\n",
    "        else:\n",
    "            pred_out.extend([np.squeeze(test_predictions[z, :, :, :]) for z in list(range(len(x_batch)))])\n",
    "        i += batch_size\n",
    "    dic_tmp= []\n",
    "    for i in range(len(y_orig)):\n",
    "        iou = get_mean_iou(pred_out[i], y_orig[i], ret_full = true)    \n",
    "        print('test iou: ', iou, 'mean: ', np.mean(iou[:output_classes-1]))\n",
    "        iou_out.append(np.mean(iou[:output_classes-1]))\n",
    "\n",
    "        \n",
    "        intersection = np.sum(y_orig[i] * pred_out[i])\n",
    "        if (np.sum(y_orig[i])==0) and (np.sum(pred_out[i])==0):\n",
    "            dic_tmp =1\n",
    "        else: \n",
    "            dic_tmp = (2*intersection) / (np.sum(y_orig[i]) + np.sum(pred_out[i]))\n",
    "        print('dice similarity: ', dic_tmp)\n",
    "       \n",
    "\n",
    "    print('mean test iou', np.mean(iou_out), 'var iou', np.var(iou_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post processing \n",
    "final_prediction = np.zeros(256,256,256)\n",
    "final_annotation =np.zeros(256,256,256)\n",
    "exam_ref = []\n",
    "pre_exam_ref = 'NA'\n",
    "\n",
    "for i in range(len(test_ref))\n",
    "    index =i\n",
    "    current_test_ref = test_ref[i]     \n",
    "    exam_ref = current_test_ref.split('_')[0] + current_test_ref.split('_')[1]\n",
    "    \n",
    "    if exam_ref == pre_exam_ref: # check patches are from the same case\n",
    "        ref_number  =  current_test_ref.split('_')[2]\n",
    "\n",
    "        row_ref = ref_number[0]\n",
    "        col_ref = ref_number[1]\n",
    "        dep_ref = ref_number[2]\n",
    "\n",
    "        row_start = 64 * (row_ref-1)\n",
    "        row_end = (row_start + 64)\n",
    "\n",
    "        col_start = 64 * (col_ref-1)\n",
    "        col_end = (col_start + 64)\n",
    "\n",
    "        dep_start =64 * (dep_ref-1)\n",
    "        dep_end = (dep_start + 64)\n",
    "\n",
    "        final_prediction [row_start:row_end,col_start: col_end,dep_start:dep_end] = pred_out[i]\n",
    "        final_annotation [row_start:row_end,col_start: col_end,dep_start:dep_end] = test[i][1][:]\n",
    "\n",
    "        \n",
    "        pre_exam_ref = exam_ref\n",
    "        \n",
    "    else:\n",
    "        final_prediction = np.zeros(256,256,256)\n",
    "        final_annotation =np.zeros(256,256,256)\n",
    "\n",
    "    save_lesion('Test Result/Prediction_3/test_',exam_ref, '-', final_prediction)\n",
    "    save_lesion('Test Result/Annotation_3/ann_',exam_ref,'_', final_annotation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
